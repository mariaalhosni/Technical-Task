{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec9a4643-eb41-4c8b-85bb-57615294d86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import Counter\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "import re\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b16aefe-dedb-48f7-a745-72e1facca1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b24020fd-0773-41ef-a0cb-77e89ffd2c66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>\\n\\nI am sure some bashers of Pens fans are pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>My brother is in the market for a high-perform...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17</td>\n",
       "      <td>\\n\\n\\n\\n\\tFinally you said what you dream abou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>\\nThink!\\n\\nIt's the SCSI card doing the DMA t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1)    I have an old Jasmine drive which I cann...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Category                                               Text\n",
       "0        10  \\n\\nI am sure some bashers of Pens fans are pr...\n",
       "1         3  My brother is in the market for a high-perform...\n",
       "2        17  \\n\\n\\n\\n\\tFinally you said what you dream abou...\n",
       "3         3  \\nThink!\\n\\nIt's the SCSI card doing the DMA t...\n",
       "4         4  1)    I have an old Jasmine drive which I cann..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
    "df_newsgroups = pd.DataFrame({\n",
    "    'Category': newsgroups.target,\n",
    "    'Text': newsgroups.data    \n",
    "})\n",
    "\n",
    "df_newsgroups.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8ea8a6b-b92f-41a8-8bd5-f7155b754cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "\n",
    "    text = text.lower()\n",
    "\n",
    "    text = ' '.join([word for word in text.split() if word not in ENGLISH_STOP_WORDS])\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eddc1dd0-f698-4954-bdb7-57ba9519daac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_newsgroups['cleaned_text'] = df_newsgroups['Text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c2eb921f-67fd-44df-8d4f-977bdb30a124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        cleaned_text  \\\n",
      "0  sure bashers pens fans pretty confused lack ki...   \n",
      "1  brother market highperformance video card supp...   \n",
      "2  finally said dream mediterranean new area grea...   \n",
      "3  think scsi card doing dma transfers disks scsi...   \n",
      "4  old jasmine drive use new understanding upsate...   \n",
      "\n",
      "                                     lemmatized_text  \n",
      "0  sure basher pen fan pretty confused lack kind ...  \n",
      "1  brother market highperformance video card supp...  \n",
      "2  finally say dream mediterranean new area great...  \n",
      "3  think scsi card dma transfer disk scsi card dm...  \n",
      "4  old jasmine drive use new understanding upsate...  \n"
     ]
    }
   ],
   "source": [
    "def lemmatize_text(text):\n",
    "    doc = nlp(text)\n",
    "    return \" \".join([token.lemma_ for token in doc if not token.is_stop and not token.is_punct])\n",
    "\n",
    "df_newsgroups['lemmatized_text'] = df_newsgroups['cleaned_text'].apply(lemmatize_text)\n",
    "\n",
    "print(df_newsgroups[['cleaned_text', 'lemmatized_text']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d09e717f-c39f-4481-b548-dae805375f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759ee4c5-440a-4c7f-a617-4359637c8361",
   "metadata": {},
   "source": [
    "## word embeddings using glove 6b 100d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cbbd5d96-b5b4-4ca7-83da-965c45160461",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_path = \"C:\\\\Users\\\\maria\\\\OneDrive\\\\Desktop\\\\glove.6B.100d.txt\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ee07ad0b-dd24-4126-862c-6b4c092c7606",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_newsgroups['tokens'] = df_newsgroups['lemmatized_text'].apply(lambda x: x.split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b107595d-ffe5-4436-8bbd-57f2a4f2d1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dict = {}\n",
    "with open(glove_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], dtype='float32')\n",
    "        embedding_dict[word] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a1839171-3b74-4764-84dc-b6046f10d6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_vector(tokens, embedding_dict):\n",
    "    word_vectors = [embedding_dict[word] for word in tokens if word in embedding_dict]\n",
    "    if word_vectors:\n",
    "        return np.mean(word_vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(100)  \n",
    "\n",
    "df_newsgroups['doc_vector'] = df_newsgroups['tokens'].apply(lambda tokens: text_to_vector(tokens, embedding_dict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "676ba694-1587-4478-b8f1-baa038ed98af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Text</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>lemmatized_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>doc_vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>\\n\\nI am sure some bashers of Pens fans are pr...</td>\n",
       "      <td>sure bashers pens fans pretty confused lack ki...</td>\n",
       "      <td>sure basher pen fan pretty confused lack kind ...</td>\n",
       "      <td>[sure, basher, pen, fan, pretty, confused, lac...</td>\n",
       "      <td>[-0.09148375, 0.1848849, 0.3459299, -0.4693587...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>My brother is in the market for a high-perform...</td>\n",
       "      <td>brother market highperformance video card supp...</td>\n",
       "      <td>brother market highperformance video card supp...</td>\n",
       "      <td>[brother, market, highperformance, video, card...</td>\n",
       "      <td>[-0.18886025, -0.020166146, -0.009497073, -0.3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17</td>\n",
       "      <td>\\n\\n\\n\\n\\tFinally you said what you dream abou...</td>\n",
       "      <td>finally said dream mediterranean new area grea...</td>\n",
       "      <td>finally say dream mediterranean new area great...</td>\n",
       "      <td>[finally, say, dream, mediterranean, new, area...</td>\n",
       "      <td>[-0.016622422, 0.2094453, 0.26692814, -0.01739...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>\\nThink!\\n\\nIt's the SCSI card doing the DMA t...</td>\n",
       "      <td>think scsi card doing dma transfers disks scsi...</td>\n",
       "      <td>think scsi card dma transfer disk scsi card dm...</td>\n",
       "      <td>[think, scsi, card, dma, transfer, disk, scsi,...</td>\n",
       "      <td>[-0.2786962, 0.04869291, 0.03302267, -0.281397...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1)    I have an old Jasmine drive which I cann...</td>\n",
       "      <td>old jasmine drive use new understanding upsate...</td>\n",
       "      <td>old jasmine drive use new understanding upsate...</td>\n",
       "      <td>[old, jasmine, drive, use, new, understanding,...</td>\n",
       "      <td>[-0.10370016, 0.13565378, 0.19263354, -0.00533...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18841</th>\n",
       "      <td>13</td>\n",
       "      <td>DN&gt; From: nyeda@cnsvax.uwec.edu (David Nye)\\nD...</td>\n",
       "      <td>dn nyedacnsvaxuwecedu david nye dn neurology d...</td>\n",
       "      <td>dn nyedacnsvaxuwecedu david nye dn neurology d...</td>\n",
       "      <td>[dn, nyedacnsvaxuwecedu, david, nye, dn, neuro...</td>\n",
       "      <td>[-0.16151656, 0.08059155, 0.19148546, 0.007797...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18842</th>\n",
       "      <td>12</td>\n",
       "      <td>\\nNot in isolated ground recepticles (usually ...</td>\n",
       "      <td>isolated ground recepticles usually unusual co...</td>\n",
       "      <td>isolate ground recepticle usually unusual colo...</td>\n",
       "      <td>[isolate, ground, recepticle, usually, unusual...</td>\n",
       "      <td>[-0.27700564, 0.15688774, 0.055569075, -0.0099...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18843</th>\n",
       "      <td>3</td>\n",
       "      <td>I just installed a DX2-66 CPU in a clone mothe...</td>\n",
       "      <td>just installed dx cpu clone motherboard tried ...</td>\n",
       "      <td>instal dx cpu clone motherboard try mount cpu ...</td>\n",
       "      <td>[instal, dx, cpu, clone, motherboard, try, mou...</td>\n",
       "      <td>[-0.4123846, 0.24455638, 0.120964006, -0.19238...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18844</th>\n",
       "      <td>1</td>\n",
       "      <td>\\nWouldn't this require a hyper-sphere.  In 3-...</td>\n",
       "      <td>wouldnt require hypersphere space points speci...</td>\n",
       "      <td>not require hypersphere space point specifie s...</td>\n",
       "      <td>[not, require, hypersphere, space, point, spec...</td>\n",
       "      <td>[-0.09420526, 0.19943206, 0.42651805, 0.169537...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18845</th>\n",
       "      <td>7</td>\n",
       "      <td>After a tip from Gary Crum (crum@fcom.cc.utah....</td>\n",
       "      <td>tip gary crum crumfcomccutahedu got phone pont...</td>\n",
       "      <td>tip gary crum crumfcomccutahedu get phone pont...</td>\n",
       "      <td>[tip, gary, crum, crumfcomccutahedu, get, phon...</td>\n",
       "      <td>[0.006206175, 0.11391043, 0.3026957, -0.117427...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18846 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Category                                               Text  \\\n",
       "0            10  \\n\\nI am sure some bashers of Pens fans are pr...   \n",
       "1             3  My brother is in the market for a high-perform...   \n",
       "2            17  \\n\\n\\n\\n\\tFinally you said what you dream abou...   \n",
       "3             3  \\nThink!\\n\\nIt's the SCSI card doing the DMA t...   \n",
       "4             4  1)    I have an old Jasmine drive which I cann...   \n",
       "...         ...                                                ...   \n",
       "18841        13  DN> From: nyeda@cnsvax.uwec.edu (David Nye)\\nD...   \n",
       "18842        12  \\nNot in isolated ground recepticles (usually ...   \n",
       "18843         3  I just installed a DX2-66 CPU in a clone mothe...   \n",
       "18844         1  \\nWouldn't this require a hyper-sphere.  In 3-...   \n",
       "18845         7  After a tip from Gary Crum (crum@fcom.cc.utah....   \n",
       "\n",
       "                                            cleaned_text  \\\n",
       "0      sure bashers pens fans pretty confused lack ki...   \n",
       "1      brother market highperformance video card supp...   \n",
       "2      finally said dream mediterranean new area grea...   \n",
       "3      think scsi card doing dma transfers disks scsi...   \n",
       "4      old jasmine drive use new understanding upsate...   \n",
       "...                                                  ...   \n",
       "18841  dn nyedacnsvaxuwecedu david nye dn neurology d...   \n",
       "18842  isolated ground recepticles usually unusual co...   \n",
       "18843  just installed dx cpu clone motherboard tried ...   \n",
       "18844  wouldnt require hypersphere space points speci...   \n",
       "18845  tip gary crum crumfcomccutahedu got phone pont...   \n",
       "\n",
       "                                         lemmatized_text  \\\n",
       "0      sure basher pen fan pretty confused lack kind ...   \n",
       "1      brother market highperformance video card supp...   \n",
       "2      finally say dream mediterranean new area great...   \n",
       "3      think scsi card dma transfer disk scsi card dm...   \n",
       "4      old jasmine drive use new understanding upsate...   \n",
       "...                                                  ...   \n",
       "18841  dn nyedacnsvaxuwecedu david nye dn neurology d...   \n",
       "18842  isolate ground recepticle usually unusual colo...   \n",
       "18843  instal dx cpu clone motherboard try mount cpu ...   \n",
       "18844  not require hypersphere space point specifie s...   \n",
       "18845  tip gary crum crumfcomccutahedu get phone pont...   \n",
       "\n",
       "                                                  tokens  \\\n",
       "0      [sure, basher, pen, fan, pretty, confused, lac...   \n",
       "1      [brother, market, highperformance, video, card...   \n",
       "2      [finally, say, dream, mediterranean, new, area...   \n",
       "3      [think, scsi, card, dma, transfer, disk, scsi,...   \n",
       "4      [old, jasmine, drive, use, new, understanding,...   \n",
       "...                                                  ...   \n",
       "18841  [dn, nyedacnsvaxuwecedu, david, nye, dn, neuro...   \n",
       "18842  [isolate, ground, recepticle, usually, unusual...   \n",
       "18843  [instal, dx, cpu, clone, motherboard, try, mou...   \n",
       "18844  [not, require, hypersphere, space, point, spec...   \n",
       "18845  [tip, gary, crum, crumfcomccutahedu, get, phon...   \n",
       "\n",
       "                                              doc_vector  \n",
       "0      [-0.09148375, 0.1848849, 0.3459299, -0.4693587...  \n",
       "1      [-0.18886025, -0.020166146, -0.009497073, -0.3...  \n",
       "2      [-0.016622422, 0.2094453, 0.26692814, -0.01739...  \n",
       "3      [-0.2786962, 0.04869291, 0.03302267, -0.281397...  \n",
       "4      [-0.10370016, 0.13565378, 0.19263354, -0.00533...  \n",
       "...                                                  ...  \n",
       "18841  [-0.16151656, 0.08059155, 0.19148546, 0.007797...  \n",
       "18842  [-0.27700564, 0.15688774, 0.055569075, -0.0099...  \n",
       "18843  [-0.4123846, 0.24455638, 0.120964006, -0.19238...  \n",
       "18844  [-0.09420526, 0.19943206, 0.42651805, 0.169537...  \n",
       "18845  [0.006206175, 0.11391043, 0.3026957, -0.117427...  \n",
       "\n",
       "[18846 rows x 6 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "94ea0f54-1442-4682-a48d-6c6a24367e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = np.vstack(df_newsgroups['doc_vector'].values)\n",
    "y = df_newsgroups['Category'] \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f03b08b0-8514-4551-bc97-dd6829331d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 32 candidates, totalling 96 fits\n",
      "Best Parameters: {'C': 10, 'gamma': 'scale', 'kernel': 'rbf'}\n",
      "Best Cross-Validation Accuracy: 0.606725980127219\n",
      "Test Accuracy: 0.6090185676392573\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "svm = SVC()\n",
    "\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],  \n",
    "    'kernel': ['linear', 'rbf'],  \n",
    "    'gamma': ['scale', 0.1, 0.01, 0.001]  \n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=svm, param_grid=param_grid, cv=3, scoring='accuracy', verbose=2, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)  \n",
    "\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Cross-Validation Accuracy:\", grid_search.best_score_)\n",
    "\n",
    "best_svm = grid_search.best_estimator_\n",
    "test_accuracy = best_svm.score(X_test, y_test)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "178f68a2-1eaf-45cc-82a8-cdd887f90bcc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 216 candidates, totalling 648 fits\n",
      "{'bootstrap': False, 'max_depth': 20, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 300}\n",
      "0.5666617964693049\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8f367ca8-265a-44a3-902e-026052736239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.5514588859416446\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gbm = GradientBoostingClassifier(n_estimators=200, learning_rate=0.1, max_depth=3)\n",
    "gbm.fit(X_train, y_train)\n",
    "print(f\"Test Accuracy: {gbm.score(X_test, y_test)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b0463a-0917-4748-ab23-e1a0adfe13dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
